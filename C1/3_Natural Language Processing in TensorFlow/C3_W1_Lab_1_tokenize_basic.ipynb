{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"C3_W1_Lab_1_tokenize_basic.ipynb","provenance":[],"authorship_tag":"ABX9TyNeztBxZUY0o4K4LYIz3CW5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z6TIsHXiQj9r","executionInfo":{"status":"ok","timestamp":1649775485817,"user_tz":-120,"elapsed":4996,"user":{"displayName":"tiziano medas","userId":"12272265020738565914"}},"outputId":"adcb7eb8-7746-44d5-cb1b-234f15e848f0"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5}\n"]}],"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","# Define input sentences\n","sentences = [\n","    'i love my dog',\n","    'I, love my cat'\n","    ]\n","\n","# Initialize the Tokenizer class\n","tokenizer = Tokenizer(num_words = 100)\n","\n","# Generate indices for each word in the corpus\n","tokenizer.fit_on_texts(sentences)\n","\n","# Get the indices and print it\n","word_index = tokenizer.word_index\n","print(word_index)"]},{"cell_type":"code","source":["# Define input sentences\n","sentences = [\n","    'i love my dog',\n","    'I, love my cat',\n","    'You love my dog!'\n","]\n","\n","# Initialize the Tokenizer class\n","tokenizer = Tokenizer(num_words = 1)\n","\n","# Generate indices for each word in the corpus\n","tokenizer.fit_on_texts(sentences)\n","\n","# Get the indices and print it\n","word_index = tokenizer.word_index\n","print(word_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C_t5TLHwQ1yS","executionInfo":{"status":"ok","timestamp":1649775498729,"user_tz":-120,"elapsed":253,"user":{"displayName":"tiziano medas","userId":"12272265020738565914"}},"outputId":"1fd82aac-6f1b-41ba-b4b4-3216641e23db"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"]}]}]}